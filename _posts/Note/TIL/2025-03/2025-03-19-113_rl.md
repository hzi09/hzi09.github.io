---
title:  "[TIL] 내일배움캠프 113일차_[ML] 강화학습 기반 추천 시스템" 

categories: 
    - TIL
tags: 
    - TIL
    - 내일배움캠프
    - 머신러닝
    - 강화학습 기반 추천 시스템


toc: True
toc_sticky: True
---

![TIL](/assets/images/TIL2.png){: .align-center style="width:35%;"}

# 👀Today I Learn
## 추천 시스템의 발전 과정
### 전통적인 추천 시스템

- **콘텐츠 기반 필터링 (Content-based Filtering)**
  - 사용자가 선호했던 아이템의 특징을 분석하여 유사한 항목을 추천하는 방식
  - 한계를 극복하기 어려운 "Cold-start" 문제 발생
    - Cold-start? : 사용자 또는 아이템에 대한 데이터가 부족할 때 발생하는 문제

- **협업 필터링 (Collaborative Filtering)**
  - 사용자의 행동 데이터(예: 좋아요, 클릭, 시청 시간)를 기반으로 유사한 사용자 또는 아이템을 찾아 추천
  - 데이터가 충분하지 않을 경우 학습이 어렵고, 변화하는 선호도를 반영하기 어려움

### 머신러닝/딥러닝 기반 추천 시스템
- 행렬 분해(Matrix Factorization), 신경망 기반 추천 시스템(Deep Learning for Recommendation, DLR) 등 다양한 기법이 등장
- 사용자의 패턴을 학습할 수 있지만, 실시간 적응력이 부족

### 강화학습을 활용한 추천 시스템의 필요성
- 기존 추천 시스템은 정적인 데이터 기반 예측이 주를 이루었으나, 사용자 행동은 동적
- 강화학습은 사용자의 지속적인 피드백을 반영하여 실시간으로 최적의 추천 전략을 학습 가능

<br>

## 강화학습(Reinforcement Learning, RL) 개요

### 강화학습의 기본 개념
- 강화학습은 "에이전트(Agent)"가 "환경(Environment)"과 상호작용하면서 보상을 최대화하는 방식으로 학습하는 알고리즘
  - **에이전트(Agent)**: 추천 시스템 (추천 모델)
  - **환경(Environment)**: 사용자가 상호작용하는 플랫폼 (예: 동영상 스트리밍, 쇼핑몰)
  - **상태(State)**: 사용자의 현재 상태 (예: 최근 본 영화, 검색 기록)
  - **행동(Action)**: 추천할 아이템 선택
  - **보상(Reward)**: 추천 결과에 대한 사용자 반응 (예: 클릭, 구매, 시청 시간 증가 등)

### 기존 추천 시스템과 강화학습의 차이점

|    구분   	|       기존 추천 시스템      	|      강화학습 기반 추천 시스템     	|
|:---------:	|:---------------------------:	|:----------------------------------:	|
| 학습 방식 	| 과거 데이터를 바탕으로 예측 	| 사용자 행동을 바탕으로 실시간 학습 	|
| 피드백    	| 정적인 데이터 분석          	| 동적인 환경에서 지속적 학습        	|
| 목적      	| 개별 아이템 예측            	| 장기적인 사용자 만족 극대화        	|

<br>

## 강화학습 기반 추천 시스템 작동 원리
### 마르코프 결정 과정 (Markov Decision Process, MDP)
- 강화학습 기반 추천 시스템은 MDP로 모델링할 수 있음
  - 상태(State): 사용자의 현재 상태 (예: 최근 클릭한 아이템)
  - 행동(Action): 추천할 아이템 선택
  - 보상(Reward): 사용자의 반응 (클릭, 좋아요, 시청 시간 등)
  - 정책(Policy): 최적의 추천 전략을 학습하는 방법

### 추천 시스템에서 활용 가능한 강화학습 기법
- Q-learning: 가치 기반 학습을 통해 최적의 행동을 선택하는 방식
- Deep Q-Networks (DQN): 신경망을 활용하여 복잡한 상태에서도 Q-learning을 확장
- Policy Gradient: 직접적으로 추천 정책을 학습하여 최적화

<br>

## 강화학습 추천 시스템 적용 사례
### Netflix, YouTube, Spotify 등에서 활용
- Netflix: 사용자 시청 패턴을 분석하여 동적인 추천
- YouTube: 클릭률과 시청 지속 시간을 고려한 최적 추천 전략
- Spotify: 사용자 피드백을 기반으로 실시간 추천 조정

### 온라인 학습(Online Learning)과 오프라인 학습(Offline Learning)
- 오프라인 학습: 과거 데이터를 활용하여 강화학습 모델 학습
- 온라인 학습: 사용자 반응을 실시간으로 반영하여 학습 진행

<br>

## 강화학습 추천 시스템의 한계점 및 해결 방법
- Cold-start 문제: 초기 사용자 데이터를 활용하여 탐색 단계를 최적화
- 탐색과 활용의 균형 (Exploration vs. Exploitation): Epsilon-greedy 등의 전략 활용
- 실시간 연산 비용 문제: 효율적인 모델 경량화 및 캐싱 기법 활용

<br>
<br>

# 💡Today I Thought

## 오늘의 체크리스트
- [x]  알고리즘 코드카타 1문제
- [x]  SQL 코드카타 2문제
- [x]  오전 : Frontend 개선
    - [ ]  header 버튼 충돌난거 수정
    - [x]  Home 화면 정리
    - [x]  마이페이지 수정 페이지 용달 블루 색상 변경
- [x]  오후 : ML 공부
- [x]  TIL 작성

## 회고
&nbsp;생각보다 어려운 개념이라서.. 조금 접어둬야할듯..ㅎㅎ 일단 공부는 해봐야겠다. 아니.. 오늘 하루종일 프론트 만졌는데 헤더 충돌을 안고쳤네.. 휴.. 1차 유저 테스트 끝나고 해야겠다. 내일부터는 ML 코드 조금씩 짜면 될듯!!!