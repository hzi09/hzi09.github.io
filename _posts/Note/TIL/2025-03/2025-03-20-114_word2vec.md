---
title:  "[TIL] 내일배움캠프 114일차_[ML] Word2Vec" 

categories: 
    - TIL
tags: 
    - TIL
    - 내일배움캠프
    - 머신 러닝
    - Word2Vec


toc: True
toc_sticky: True
---

![TIL](/assets/images/TIL2.png){: .align-center style="width:35%;"}

# 👀Today I Learn
## 1. Word2Vec이란?

- Word2Vec은 구글이 2013년에 발표한 단어 임베딩 기법으로, 단어를 고정된 크기의 벡터로 변환하는 방법
- 벡터들은 의미적 유사성을 반영하도록 훈련되며, 기계 학습 모델에서 자연어를 더 효과적으로 다룰 수 있도록 돕움
- Word2Vec은 신경망을 기반으로 단어 간 의미적 관계를 벡터 공간에 학습시키는 방식으로 작동하며, 대표적으로 **CBOW(Continuous Bag of Words)** 및 **Skip-gram** 모델을 활용

<br>

## 2. Word2Vec의 핵심 아이디어

- Word2Vec은 "단어의 의미는 주변 단어(context)로 결정된다"는 분포 가설(Distributional Hypothesis)을 기반으로 함
- 즉, 의미적으로 비슷한 단어들은 유사한 문맥에서 등장한다는 가정
- 이 가설을 바탕으로 단어를 벡터 공간에서 가깝게 배치하여 의미적 관계를 수치적으로 표현할 수 있음

<br>

## 3. Word2Vec의 학습 방법

### (1) CBOW (Continuous Bag of Words)

- CBOW는 특정 단어를 주변 단어들의 문맥을 기반으로 예측하는 방식
    - 예시:  `The`, `sits`, `on`, `the`와 같은 주변 단어들을 이용하여 `cat`을 예측하도록 학습
- 여러 개의 주변 단어를 활용하여 중심 단어를 예측하기 때문에 연산이 효율적이며, 속도가 빠름
- 단점으로는 희소한 단어에 대한 학습이 어려울 수 있음

### (2) Skip-gram

- Skip-gram은 중심 단어를 주어진 상태에서 주변 단어들을 예측하는 방식
    - 예시: `cat`이 중심 단어일 때, Skip-gram 모델은 `The`, `sits`, `on`, `the`, `mat` 등의 주변 단어를 예측하는 방식으로 학습
- CBOW보다 데이터가 적을 경우에도 높은 성능을 보이며, 희소한 단어에도 강함
- 다만, 학습 속도가 CBOW보다 느릴 수 있음
    - 예측해야 하는 출력 개수가 많아서 연산량이 증가
    - 중심 단어 하나당 여러 개의 주변 단어를 예측해야 하므로 더 많은 샘플을 생성해야 함
    - 각 중심 단어마다 주변 단어에 대한 확률 분포를 계산해야 하는데, 보통 Softmax(확률을 계산하는 함수)를 사용하면 전체 단어 사전(Vocabulary)의 크기만큼 확률을 계산해야 함

<br>

## 4. Word2Vec의 학습 과정

- Word2Vec은 입력 단어를 **원-핫 인코딩**으로 변환한 후, 신경망을 이용하여 단어 임베딩을 학습
    1. 입력 단어를 원-핫 인코딩으로 변환
        - 예: `king` → `[0, 0, 0, ..., 1, ..., 0]`
    2. 은닉층(embedding layer)에서 낮은 차원의 밀집 벡터(dense vector)로 변환
        - 예: `king` → `[0.21, -1.34, 0.98, ..., -0.45]`
    3. Softmax를 사용하여 주변 단어의 확률을 예측
    4. 오차 역전파(Backpropagation) 및 SGD(Stochastic Gradient Descent)로 모델 업데이트
- 이 과정을 반복하여 단어 벡터가 의미적으로 유사한 단어를 가까운 벡터로 변환하게 됨

<br>

## 5. Word2Vec의 응용

### (1) 단어 유사도 계산

- Word2Vec은 단어 간 유사도를 계산할 수 있음
- 예를 들어:' `king`과 `queen`은 의미상으로 비슷한 단어이므로, 이 두 단어의 벡터는 가까운 값으로 학습됨

### (2) 단어 벡터 연산

- Word2Vec의 가장 강력한 기능 중 하나는 벡터 연산
- 예를 들어 : `king - man + woman ≈ queen` 이라는 관계가 성립함

### (3) 클러스터링 및 차원 축소

- 단어 벡터를 활용하여 의미적으로 유사한 단어들을 그룹화할 수 있음
- t-SNE 같은 차원 축소 기법을 이용하여 시각화 가능

<br>

## 6. 한계점 및 해결방안

### (1) 문맥을 고려하지 않음

- Word2Vec은 단어를 **고정된 벡터**로 표현하기 때문에, 단어가 어떤 문맥에서 사용되었는지에 따른 의미의 변화를 반영할 수 없음
    - 예를 들어, "Crane"이라는 단어가 등장하면, 이는 "새"일 수도 있고, "기중기"일 수도 있음
    - 하지만 Word2Vec은 이 두 경우를 구별하지 못하고 **하나의 고정된 벡터**로 "Crane"을 표현
- 자연어에서 단어는 종종 여러 의미를 가질 수 있는데, Word2Vec은 이 점을 고려하지 않기 때문에 **다의어**를 처리하는 데 한계가 있음
- 해결 방안
    - 이런 한계를 극복하기 위해 **BERT**와 같은 문맥을 고려한 모델들이 등장
        - BERT는 문맥에 따라 단어의 의미가 달라지는 것을 반영하여 **동적**으로 단어 벡터를 생성
        - 이를 통해 다의어나 문맥에 따른 의미 차이를 보다 잘 처리할 수 있음

### (2) 대규모 데이터 요구

- Word2Vec은 모델이 **좋은 품질의 단어 벡터**를 학습하려면 매우 많은 텍스트 데이터가 필요
- 텍스트 데이터가 적을 경우, 각 단어의 의미를 잘 학습하지 못하거나 **희소한 단어**에 대한 벡터 품질이 떨어질 수 있음
    - **통계적 접근**을 사용하여 단어 간의 관계를 학습하기 때문에, 대규모 데이터셋이 필요
    - 적은 양의 데이터로 학습하면, 특히 **희소한 단어**(자주 등장하지 않는 단어)에 대한 학습이 어려워짐
    - 예를 들어, 특정 전문 용어나 드물게 사용되는 단어의 벡터는 잘 학습되지 않을 수 있음
- 해결방안
    - 데이터가 충분하지 않은 경우, **전이 학습**(Transfer Learning)을 이용하는 방법이 있음
    - 이미 대규모 텍스트 데이터로 학습된 모델을 기반으로 특정 도메인에 맞게 추가 학습을 시킬 수 있음
    - 또한, **FastText** 같은 모델은 **단어의 서브워드** 정보를 활용하여 더 적은 데이터로도 유용한 벡터를 얻을 수 있음

### (3) Out-of-Vocabulary(OOV) 문제

- Word2Vec은 **학습된 단어만** 벡터로 변환할 수 있기 때문에 학습 데이터에 등장하지 않은 단어는 모델이 처리할 수 없음
    - 예를 들어, 새로운 단어나 특수한 용어가 등장했을 때, 이 단어를 벡터로 변환할 방법이 없음
- 실제 자연어에는 지속적으로 새로운 단어들이 등장하는데, Word2Vec은 **사전**에 포함된 단어만 처리할 수 있음
    - 예를 들어, "cryptocurrency"나 "self-driving car" 같은 신조어나 최근의 기술적 용어들은 Word2Vec 모델에 포함되지 않았다면 벡터로 변환할 수 없음
- 해결방안
    - **FastText**는 이 문제를 해결하는 좋은 예시
        - FastText는 단어를 서브워드 단위로 분할하여 학습하기 때문에, 학습되지 않은 단어라도 **서브워드 벡터**를 기반으로 예측할 수 있음
        - 예를 들어, "unhappiness"라는 단어가 학습되지 않았다 하더라도, FastText는 "un", "happi", "ness" 등의 서브워드를 사용해 벡터를 계산할 수 있음

<br>
<br>

# 💡Today I Thought

## 오늘의 체크리스트
- [x]  알고리즘 코드카타 1문제
- [x]  SQL 코드카타 1문제
- [x]  1차 유저테스트 공지 올리기
    - [x]  유저테스트를 위한 마지막 점검 진행
- [x]  브로셔 중간중간 작성하기 - 주말 내에 완성할 예정
- [x]  SA 문서 보충
- [x]  ML 공부하기
- [x]  TIL 작성

## 회고
&nbsp;오늘은 1차 유저테스트를 진행한 날..👍 어제 밤에 진짜 자신이 없어서 그냥 유저테스트 안하면 안되냐고 했는데, 아침에 이것저것 트러블슈팅 해결하고 테스트 진행할 수 있어서 다행이다. 아직 잘 되는건 없지만..🤔