---
title:  "[TIL] 내일배움캠프 102일차_[ML] TF-IDF 벡터화" 

categories: 
    - TIL
tags: 
    - TIL
    - 내일배움캠프
    - 머신러닝
    - TF-IDF 벡터화


toc: True
toc_sticky: True
---

![TIL](/assets/images/TIL2.png){: .align-center style="width:35%;"}

# 👀Today I Learn
## TF-IDF란?
- TF-IDF(Term Frequency-Inverse Document Frequency)는 문서에서 중요한 단어를 찾고, 이를 수치화하여 표현하는 방법
- 자연어 처리(NLP)에서 문서를 벡터화할 때 자주 사용됨

### TF(Term Frequency) - 단어 빈도수
- TF는 특정 단어가 문서에서 얼마나 자주 등장하는지를 나타냄

    $$
    TF(t) = \frac{\text{특정 단어 t의 문서 내 등장 횟수}}{\text{문서의 총 단어 수}}
    $$

- 즉, 특정 단어가 문서에서 많이 등장할수록 값이 커짐

### IDF(Inverse Document Frequency) - 역문서 빈도수
- IDF는 단어의 중요도를 조절하기 위해 사용됨
- 만약 모든 문서에서 공통으로 등장하는 단어(예: "the", "is")가 있다면, 이러한 단어의 가중치를 낮추기 위해 IDF를 사용함

    $$
    IDF(t) = \log \frac{N}{DF(t)}
    $$

  - 여기서:
    - N = 전체 문서의 수
    - DF(t) = 특정 단어 t를 포함하는 문서의 수

- IDF 값이 크면 드문 단어이며, 값이 작으면 일반적인 단어


### TF-IDF 계산
- TF와 IDF를 곱하여 최종적으로 TF-IDF 값을 구함

    $$
    TF-IDF(t) = TF(t) \times IDF(t)
    $$

- 이렇게 하면 자주 등장하지만 모든 문서에 존재하는 흔한 단어의 중요도를 낮추고, 특정 문서에서만 중요한 단어를 강조할 수 있음

<br>

## TF-IDF 벡터화 예제
Python의 `sklearn.feature_extraction.text.TfidfVectorizer`를 사용하여 직접 TF-IDF 벡터화를 구현해보기

### 데이터 준비

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# 샘플 문서 데이터
documents = [
    "I love machine learning and natural language processing.",
    "Machine learning is a method of data analysis.",
    "Natural language processing and deep learning are key techniques in AI.",
]
```


### TF-IDF 벡터화

```python
# TF-IDF 벡터화
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# 결과를 데이터프레임으로 변환
feature_names = vectorizer.get_feature_names_out()
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)
print(df_tfidf)
```

### 출력 결과
- 위 코드를 실행하면 다음과 같은 TF-IDF 행렬이 생성됨


|   | ai  | analysis | and  | data | deep | in  | is  | key | language | learning | love | machine | method | natural | of  | processing | techniques |
|---|----|---------|-----|------|------|----|----|-----|---------|----------|------|---------|--------|---------|----|-----------|------------|
| 문서1 | 0  | 0       | 0.38 | 0    | 0    | 0  | 0  | 0   | 0.38     | 0.38     | 0.54 | 0.38    | 0      | 0.38     | 0  | 0.38      | 0          |
| 문서2 | 0  | 0.44    | 0   | 0.44 | 0    | 0  | 0.44 | 0   | 0        | 0.33     | 0   | 0.33    | 0.44   | 0        | 0.44 | 0        | 0          |
| 문서3 | 0.33 | 0      | 0.26 | 0    | 0.33 | 0.33 | 0  | 0.33 | 0.26     | 0.26     | 0   | 0.26    | 0      | 0.26     | 0  | 0.26      | 0.33       |

- 위 테이블에서 각 값은 해당 단어의 TF-IDF 값을 나타내며, 특정 문서에서 중요도가 높은 단어일수록 값이 큼

<br>

## TF-IDF의 장점과 한계
### ✅장점
- **단순하지만 강력함**: 비교적 쉬운 개념이지만 문서의 중요한 단어를 효과적으로 찾을 수 있음
- **불필요한 단어 제거**: 흔한 단어의 중요도를 낮추기 때문에 핵심 단어를 더 강조할 수 있음
- **벡터 연산 가능**: 문서를 벡터로 변환하므로 머신러닝, 검색 엔진 등에서 쉽게 활용할 수 있음

### 🔒한계
- **문맥 정보 부족**: 단어의 순서나 의미를 반영하지 않음 (예: "not good"과 "good"을 구분하지 못함)
- **동의어 처리 미흡**: 같은 의미의 단어(예: "car" vs. "automobile")를 다르게 취급함
- **문서 길이에 따라 영향 받음**: 긴 문서에서 단어 빈도가 많아질 수 있음

<br>

## TF-IDF의 활용
- TF-IDF는 검색 엔진, 문서 분류, 뉴스 기사 추천, 감성 분석 등의 다양한 NLP 작업에 사용됨
  - 검색 엔진: 사용자의 검색어와 문서의 TF-IDF 벡터를 비교하여 관련 문서를 추천
  - 문서 분류: 특정 주제에 해당하는 키워드의 TF-IDF 값을 이용하여 문서를 분류
  - 챗봇 및 텍스트 마이닝: 중요한 단어를 기반으로 의미를 분석하고 자동 응답을 생성

<br>
<br>

# 💡Today I Thought

## 오늘의 체크리스트
- [x] 알고리즘 코드카타 1문제
- [x] SQL 코드카타 1문제
- [x] ML 공부
- [x] TIL 작성

## 회고
&nbsp;오늘은 안경도 맞추고 조금 쉬었다. 내일부턴 다시 달려야지.. 집안일도 자꾸 미루는 미룬이..😂