---
title:  "[TIL] 내일배움캠프 77일차_[DL] 인공신경망(ANN)" 

categories: 
    - TIL
tags: 
    - TIL
    - 내일배움캠프
    - DL
    - 인공신경망
    - ANN


toc: True
toc_sticky: True
---

![TIL](/assets/images/TIL2.png){: .align-center style="width:35%;"}

# 👀Today I Learn
## 1. 인공 신경망(ANN)이란?

![Image](https://github.com/user-attachments/assets/0f0d6a80-41bb-4a80-b1f2-09f1e6be49e5){: .align-center style="width:75%;"}

- 인공 신경망(Artificial Neural Network, ANN)은 인간의 뇌 구조에서 영감을 받아 만들어진 머신러닝 모델
- ANN은 뉴런(Neuron)이라는 기본 단위를 사용하여 정보를 전달하고, 학습 데이터를 분류하거나 예측하는 능력을 가짐

### 신경망의 개념

- 인간의 뇌에는 수많은 뉴런(Neuron)이 존재하며, 뉴런들은 서로 연결되어 정보를 전달
- 인공 신경망(ANN)은 이러한 개념을 수학적으로 모델링하여 컴퓨터에서 학습할 수 있도록 만든 것

<br>

## 2. ANN의 기본 구조

![Image](https://github.com/user-attachments/assets/7e16f820-7f46-4506-919a-a186cbaf7b4c){: .align-center style="width:75%;"}

- ANN은 입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성

### 입력층(Input Layer)

- 데이터를 입력받는 부분

### 은닉층(Hidden Layer)

- 입력을 처리하고 패턴을 학습하는 층
- 뉴런(Neuron)들이 존재하며, 각 뉴런은 가중치(Weight)와 활성화 함수(Activation Function)를 가짐
- 은닉층이 많아질수록 복잡한 패턴을 학습할 수 있음(딥러닝의 핵심)

### 출력층(Output Layer)

- 최종 예측 값을 출력하는 부분

<br>

## 3. ANN의 동작 원리

### 가중치(Weight)와 바이어스(Bias)

$$
y = w_1x_1+w_2x_2+...+w_nx_n+b
$$

- x : 입력 데이터
- w : 가중치 (데이터가 얼마나 중요한지를 나타냄)
- b : 바이어스 (모델이 좀 더 유연하게 조정될 수 있도록 추가됨)

### 활성화 함수(Activation Function)

- 가중치를 적용한 후, **비선형성(Non-linearity)** 을 추가하기 위해 활성화 함수를 사용함
- 주요 활성화 함수
    - 시그모이드(Sigmoid)
        - 시그모이드 함수는 대표적인 비선형 활성화 함수로서 입력값을 0과 1 사이의 값으로 변환
        - 시그모이드 함수는 계산이 쉽고 해석이 용이해 가장 기본적으로 사용되는 활성화 함수
            
            $$
            f(x) = \frac{1}{1+e^{-x}}
            $$
            
        - 출력 범위: (0, 1)
        - 장점: 확률적인 출력을 만들 때 유용
        - 단점: 큰 값에서 기울기(Gradient)가 거의 0에 가까워져 학습이 잘 안 됨 (Vanishing Gradient 문제)
    
    - 렐루 (ReLU, Rectified Linear Unit)
        - 은닉층에서 가장 많이 사용되는 활성화 함수, 입력값이 0을넘으면 그 입력을 그대로 출력하고, 0 이하일 경우는 0을 출력해주는 함수
        - 0을 출력하는 경우를 제외하고, 강제적인 수치 변환이 이루어지지 않으므로 학습이 훨씬 빨라지며, 비용이 크지 않고 구현이 매우 간단하다는 장점이 있음
        
        $$
        f(x) = max(0, x)
        $$
        
        - 출력 범위: (0, ∞)
        - 장점: 깊은 신경망에서도 잘 작동하며, Vanishing Gradient 문제를 해결
            - Vanishing Gradient : 기울기가 0이 되어서 경사하강법을 사용할 수 없게 되는 문제
        - 단점: 입력값이 0 이하이면 뉴런이 죽어버림 (**Dead Neuron 문제**)
    
    - 소프트맥스 (Softmax)
        - 출력값을 확률로 변환하는 함수
        - 주로 분류 문제(Classification)에서 사용되며, 여러 개의 클래스(다중 클래스 분류) 중에서 가장 가능성이 높은 클래스를 예측할 때 유용
        
        $$
        f(x_i) = \frac{e^{x_i}}{\sum e^{x_j}}
        $$
        
        - 소프트맥스 함수는 입력값을 **0~1 사이의 값**으로 변환하며, 모든 출력값의 합은 항상 **1**
        - 출력값을 확률 분포처럼 해석할 수 있도록 만듦

### 순전파 (Forward Propagation)

![Image](https://github.com/user-attachments/assets/0c69463d-5ef2-43c9-a7bf-6afe9c88f0b1){: .align-center style="width:35%;"}

- 신경망에서 입력 데이터가 출력층까지 전달되는 과정을 의미
    
    $$
    y = f(wX+b)
    $$
    
    - 신경망(Neural Network)에서 입력층(Input Layer) → 은닉층(Hidden Layer) → 출력층(Output Layer) 으로 정보가 전달
    - 각 뉴런은 입력값을 받아 가중치(Weight)를 곱하고 바이어스(Bias)를 더한 후, 활성화 함수(Activation Function)를 적용하여 다음 층으로 전달
    - 최종적으로 출력층에서 예측값을 계산

### 역전파 (Back Propagation)

![Image](https://github.com/user-attachments/assets/f34487e2-5c17-4048-8bec-b0db789496e1){: .align-center style="width:35%;"}

- 신경망이 학습하는 핵심 과정
- 가중치를 업데이트하기 위해 **오차(Gradient)를 역으로 전파**하는 알고리즘
- 경사 하강법(Gradient Descent) 또는 **Adam Optimizer**를 이용하여 최적의 가중치를 찾음

<aside>
📝 일반적인 학습 4단계

1. 입력 데이터를 모델에 넣고 예측값을 구하는 과정
2. 손실 계산 : 실제값과 예측값의 차이를 계산하는 과정
3. 역전파 : 손실을 줄이기 위해서 각각의 가중치가 얼마나 영향을 미쳤는지 계산하는 과정
    - loss.backward() : 자동으로 역전파 계산
    - 가중치가 얼마나 중요한지(기울기,  Gradient) 계산
4. 가중치 업데이트
</aside>

<br>

## 4. ANN의 종류

### 심층 신경망 (Deep Neural Network, DNN)

![Image](https://github.com/user-attachments/assets/8a0b4d49-6d96-4662-a8e6-de85663c3717){: .align-center style="width:75%;"}

- 은닉층이 여러 개 있는 신경망
- 딥러닝(Deep Learning)이라고도 부름

### 합성곱 신경망 (Convolutional Neural Network, CNN)

![Image](https://github.com/user-attachments/assets/c0c16ce0-278b-4303-9c16-f6238351eb06){: .align-center style="width:75%;"}

- 이미지 인식 및 컴퓨터 비전에 최적화된 신경망
- 필터(Convolution)를 이용하여 패턴을 자동으로 학습함

### 순환 신경망 (Recurrent Neural Network, RNN)

![Image](https://github.com/user-attachments/assets/c60efa53-8e6f-4520-b50a-e740f151c5b4){: .align-center style="width:75%;"}

- 시간에 따라 변하는 데이터를 학습하는 신경망
- 텍스트, 음성, 주식 예측 등에서 사용됨

<br>

## 5. ANN의 활용 사례

- 이미지 인식 (Image Classification) : 자율 주행, 의료 영상 분석
- 자연어 처리 (NLP) : 챗봇, 번역, 감정 분석
- 음성 인식 (Speech Recognition) : AI 비서
- 추천 시스템 (Recommendation System) : 넷플릭스, 유튜브, 쇼핑몰 추천 알고리즘
- 의료 데이터 분석 : 질병 예측, 유전자 연구

<br>

## 6. ANN의 한계점

1. 많은 데이터가 필요함 
  - 데이터가 적으면 과적합(Overfitting) 발생
    
2. 학습 속도가 느림 
  - GPU 사용으로 해결 가능
    
3. 블랙박스 문제 
  - 결과가 왜 그렇게 나왔는지 해석하기 어려움 (Explainable AI 필요)

<br>
<br>

# 💡Today I Thought

## 오늘의 체크리스트
- [x]  알고리즘 코드카타 326
- [x]  SQL 코드카타 112
- [x]  스탠다드반 과제
- [x]  Streamlit 복습
- [x]  TIL 작성

## 회고
&nbsp;공부한 내용에 대해서 발표했는데, 너무 긴장되서 토 나오는 줄 알았다🫠 잠깐 공부한 걸 발표하는 건 잘 모르는 거라 더 긴장되는 거 같다. 열심히 공부해서 나중에 면접때는 안 떨어야지ㅠㅠ 내일 과제 발제..! 나는 말하는 감자🥔인데 할 수 있을까?

![Image](https://github.com/user-attachments/assets/83124d98-5397-4777-ba42-0f15a8e3fb46){: .align-center style="width:10%;"}