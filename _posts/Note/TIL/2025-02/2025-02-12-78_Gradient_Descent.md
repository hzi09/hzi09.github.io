---
title:  "[TIL] 내일배움캠프 00일차_[ML/DL] 경사 하강법(Gradient Descent)" 

categories: 
    - TIL
tags: 
    - TIL
    - 내일배움캠프
    - 머신러닝
    - 딥러닝
    - 경사 하강법


toc: True
toc_sticky: True
---

![TIL](/assets/images/TIL2.png){: .align-center style="width:35%;"}

# 👀Today I Learn
## 1. 경사 하강법(Gradient Descent)

- 경사 하강법은 최적의 해답(최적의 가중치)을 찾기 위해 사용되는 알고리즘
- 쉽게 말하면 어떻게 하면 예측이 더 정확해질까?를 고민하며 조금씩 값을 조정하는 과정

### ⛰️산에서 내려온다고 가정해보기

- 산 꼭대기(높은 비용)에서 출발해서, 가장 낮은 곳(최적의 비용)으로 내려오는 과정과 같음
- 경사가 가파른 방향으로 내려가야 빠르게 도착할 수 있음
- 한 번에 너무 많이 이동하면(학습률이 너무 크면) 목표 지점을 지나칠 수도 있음
- 너무 조금씩 이동하면(학습률이 너무 작으면) 시간이 오래 걸림

<br>

## 2. 경사 하강법의 원리

1. 모델이 데이터를 보고 예측
2. 예측이 얼마나 틀렸는지(오차, 비용)를 계산
3. 비용을 줄이기 위해 가중치를 조금씩 조정
4. 오차가 최소가 될 때까지 반복

→ 이러한 과정을 거치면서 모델이 점점 더 정확한 예측을 함

<br>

## 3. 최적의 가중치 찾는 법

### 비용 함수(Loss Function)

- 비용 함수는 "얼마나 틀렸는지" 를 계산하는 함수
    - 예를 들어, 예측값이 70점인데 실제값이 100점이라면, 오차는 30점
    - 비용 함수는 이 오차를 숫자로 계산하여 줄이는 역할을 함
- 대표적인 비용 함수는 평균 제곱 오차(MSE, Mean Squared Error)
- 비용 함수가 작을수록 모델이 더 정확한 예측을 한다는 의미

### 기울기(Gradient) 계산

- 기울기(Gradient)가 큰 방향으로 이동하면서 비용을 줄임
    - 예를 들어, 우리가 산을 내려올 때, 경사가 가파른 쪽으로 가야 빠르게 내려갈 수 있음
    - 수학적으로는 미분(∂, 기울기 계산) 을 이용해서 "어느 방향으로 이동해야 할지"를 결정
- 기울기가 양수(+)이면 → 가중치를 줄여야 함(왼쪽으로 이동)
- 기울기가 음수(-)이면 → 가중치를 증가시켜야 함(오른쪽으로 이동)

### 가중치 업데이트 (Weight Update)

- 기울기 방향을 따라 조금씩 가중치를 조정합니다.
    
    $$
    W = W - \alpha \times \text{기울기}
    $$
    
- 여기서 α는 학습률(Learning Rate) ⇒ 가중치를 이동하는 크기
    - 한 번에 너무 많이 이동하면 오버슈팅(목표를 지나침)
    - 너무 조금씩 이동하면 시간이 오래 걸림

⇒ 즉, 기울기(Gradient)를 따라 조금씩 이동하면서, 비용을 최소화하는 방향으로 가중치를 조정하는 것이 경사 하강법

<br>

## 4. 경사하강법의 종류

### 배치 경사 하강법(Batch Gradient Descent)

- 모든 데이터를 한 번에 사용하여 가중치를 업데이트
- 학습 속도가 느리지만 안정적
- 데이터가 많아지면 연산량이 커져 느려질 수 있음

- 예시 : 학생 100명의 성적을 보고 공부시간과 점수 사이의 관계를 찾는 모델
    - 100명의 모든 데이터를 이용해서 한 번에 가중치를 업데이트 하는 방식

- 장점
    - 데이터 전체를 사용하므로 매우 정확한 방법으로 가중치를 업데이트 할 수 있음
    - 수렴(최적의 값에 도달)할 때 안정적
- 단점
    - 데이터가 많아지면 계산량이 너무 많아져 학습 속도가 느려짐
    - 데이터가 많을 경우 메모리를 많이 사용해야 함

### 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)

- 한 개의 데이터만 사용하여 가중치를 업데이트
- 속도가 빠르지만, 값이 많이 흔들릴 수 있음
- 큰 데이터셋에서 유용

- 예시 : 학생 100명의 성적을 보고 공부시간과 점수 사이의 관계를 찾는 모델
    - 학생 한 명의 점수를 확인하고 즉시 가중치를 업데이트
    - 그다음 다른 학생 한 명을 확인하고 다시 가중치를 업데이트하는 방식

- 장점
    - 한 번의 업데이트에 한 개의 데이너만 사용하므로 계산이 빠름
    - 실시간 학습 가능 → 새로운 데이터가 계속 들어올 때 유용
- 단점
    - 업데이트 할 때마다 값이 흔들려서 최적해 주변에서 진동할 수 있음
    - 최적의 가중치 근처에서 안정적으로 멈추지 않을 수도 있음

### 미니 배치 경사 하강법 (Mini-Batch Gradient Descent)

- 데이터를 작은 그룹(미니 배치)으로 나누어 학습
- 속도와 안정성의 균형이 좋음
- 딥러닝에서 가장 많이 사용됨

- 예시 : 학생 100명의 성적을 보고 공부시간과 점수 사이의 관계를 찾는 모델
    - 한번에 10명씩 묶어서 가중치를 업데이트 하는 방식
    - 전체 데이터를 사용하지 않기 때문에 속도가 빠르고 안정적임

- 장점
    - SGD보다 덜 흔들리고, BGD보다 빠름
    - 딥러닝에서 가장 많이 사용되는 방식
    - 메모리 사용량이 적으면서도, 빠르고 안정적인 학습 가능
- 단점
    - 미니 배치 크기를 잘못 설정하면 학습 성능이 떨어질 수 있음
    - 미니 배치 크기가 너무 작으면 SGD처럼 불안정해지고, 너무 크면 BGD처럼 느려질 수 있음


<br>

- **미니 배치 경사 하강법**은 **배치하강법**과 **확률적 경사하강법**의 장점을 적절히 조합한 방법
- 전체 데이터를 여러 개의 작은 그룹(미니 배치)으로 나눠서 학습
- 한 번 업데이트할 때 여러 개의 데이터를 사용하지만, 모든 데이터를 한꺼번에 사용하지는 않음
{: .notice--info}

### 정리

![Image](https://github.com/user-attachments/assets/caf942d8-43ed-4a49-837c-7294b9056e0a){: style="width:50%;"}

| **방법** | **데이터 사용량** | **학습 속도** | **안정성** |
| --- | --- | --- | --- |
| **배치 경사 하강법(BGD)** | 전체 데이터 | 느림🐢 | 매우 안정적 |
| **확률적 경사 하강법 (SGD)** | 1개 데이터 | 매우 빠름🔥 | 흔들림 |
| **미니 배치 경사 하강법(MGD)** | 일부 데이터(미니 배치) | 빠름🐇 | 안정적 |

<br>
<br>

# 💡Today I Thought

## 오늘의 체크리스트
- [x]  알고리즘 코드카타 350
- [x]  SQL 코드카타 111
- [x]  스탠다드반 발표 준비
- [x]  TIL 작성

## 회고
&nbsp;오늘은 팀 프로젝트 발제가 있었다. 아직 하나도 모르겠는데 잘할 수 있으려나 몰라🫠 또.. 장고때처럼 어떻게든 노력하면 할 수 있을거야💪