---
title:  "[TIL] 내일배움캠프 60일차_[DL] Transformer 모델" 

categories: 
    - TIL
tags: 
    - TIL
    - 내일배움캠프
    - 딥러닝
    - Transformer


toc: True
toc_sticky: True
---

![TIL](/assets/images/TIL2.png){: .align-center style="width:35%;"}

# 👀Today I Learn
## 1. Transformer 모델이란?
![Image](https://github.com/user-attachments/assets/3efff466-c3bb-46cd-a8e7-44eebf3cc8b0)
- Transformer는 2017년 구글이 [**"Attention is All You Need"**](https://arxiv.org/abs/1706.03762)라는 논문에서 발표한 딥러닝 모델
- 주로 자연어 처리(Natural Language Processing, NLP) 분야에서 혁신을 일으켰지만, 현재는 컴퓨터 비전, 음성 인식 등 다양한 분야에서 활용되고 있음
- 기존의 순환 신경망(Recurrent Neural Network, RNN)이나 장단기 메모리(Long Short Term Memory, LSTM), 게이트 순환 유닛(Gated Recurrent Unit, GRU) 모델과 달리, Transformer는 '셀프 어텐션(Self-Attention)'이라는 이름의 메커니즘을 도입했음 
  - 이를 통해 병렬 처리가 가능해졌고, 장거리 의존성(long-range dependency) 문제를 해결할 수 있게 됨

 

- 쉽게 말해, Transformer는 문장의 의미를 이해하고 생성하는 데 탁월한 능력을 가진 AI 모델
- 마치 우리가 문장을 읽을 때 각 단어의 관계를 파악하고 전체적인 맥락을 이해하는 것처럼, Transformer도 비슷한 방식으로 작동한다고 볼 수 있음

<br>

## 2. 기존 모델과의 차이점
- Transformer가 등장하기 전, NLP 분야에서는 주로 RNN, LSTM, GRU 등의 모델이 사용되었음
  - RNN
    - 순차적 데이터를 처리하기 위해 설계된 신경망으로, 이전 단계의 정보를 현재 단계로 전달해 계산에 활용
    - 하지만 먼 과거의 정보를 효과적으로 활용하지 못하는 한계를 갖고 있음

  - LSTM
    - RNN의 장거리 의존성 한계를 해결하기 위해 개발된 모델
    - 정보를 선택적으로 기억하고 잊을 수 있는 구조로 긴 시계열 데이터의 예측 정확도를 높임

  - GRU
    - LSTM을 단순화한 모델로, LSTM과 비슷한 성능을 보이면서 계산 효율성을 개선

- 차이점 

    | 구분  | 기존 모델  | Transformer  |
    |:--:|:-:|:-:|
    | 데이터 처리 방식  | RNN, LSTM, GRU는 순차적으로 데이터를 처리(순차적 처리)  | 문장의 모든 단어를 동시에 처리 가능 (병렬 처리) |
    | 장거리 의존성  | 문장이 길어질수록 먼 거리에 있는 단어 간의 관계를 포착하기 어려워짐 <br> LSTM과 GRU는 이 문제를 어느 정도 개선했지만, 여전히 한계가 있음 | 셀프 어텐션 메커니즘을 통해 문장 내 모든 단어 간의 관계를 직접적으로 계산하므로 장거리 의존성 문제를 효과적으로 해결  |
    | 계산 복잡도  | 순차적 처리로 인해 문장 길이에 비례하여 계산 복잡도가 증가 <br> 즉, 문장이 길어질수록 처리 시간이 크게 늘어남 | 병렬 처리가 가능해 문장 길이에 덜 민감 <br> 다만, 셀프 어텐션 연산의 복잡도는 문장 길이의 제곱에 비례  |
    | 메모리 사용  | RNN 계열 모델은 이전 상태를 기억하기 위한 내부 메모리를 사용 <br> 특히 LSTM은 장기 기억을 위한 복잡한 게이트 구조를 가짐  | 별도의 메모리 구조 없이 어텐션 메커니즘만으로 정보를 처리  |

<br>

## 3. 셀프 어텐션(Self-Attention)
- Transformer의 가장 큰 특징은 '셀프 어텐션' 메커니즘

    ![Image](https://github.com/user-attachments/assets/cc6e68d5-2843-430d-888f-ba4e867eb063)

- "Self Attention"이란 말 그대로 Attention을 자기 자신한테 취한다는 것
- 문장에서의 단어들의 연관성을 알기위해서 사용


### 셀프 어텐션의 구조
1. **쿼리(Query), 키(Key), 값(Value) 벡터**

    ![Image](https://github.com/user-attachments/assets/484aeb8e-6880-43c6-9b02-14f27285973a)

   - 각 단어는 세 가지 벡터로 변환: 쿼리, 키, 값
     - '무엇을 찾는지 - 쿼리'
     - '어떤 정보를 가지고 있는지 - 키 '
     - '실제로 어떤 정보를 전달하는지 - 값'


2. **어텐션 점수 계산**

    ![Image](https://github.com/user-attachments/assets/91820e8c-3f25-4ef4-bb2b-935792d67c2d)

   - 각 단어의 쿼리 벡터와 다른 모든 단어의 키 벡터 사이의 내적(Dot Product)을 계산
     - Dot Product - 두 벡터가 얼마나 유사한지, 또는 얼마나 같은 방향을 가리키는지를 나타내는 지표
   - 이 점수는 해당 단어가 다른 단어들에 얼마나 '어텐션'해야 하는지 그 수치적인 정도를 나타냄


3. **소프트맥스 적용**

   - 계산된 점수에 소프트맥스 함수(숫자를 0과 1 사이의 값으로 바꾸고, 모든 값의 합이 1이 되도록 만드는 함수)를 적용하여 확률 분포로 변환
   - 이 과정을 통해 원래 큰 값은 더 큰 확률을 갖게 되고, 작은 값은 더 작은 확률을 갖게 됨
     - 즉, 중요한 관계는 더 강조되고, 덜 중요한 관계는 약화되어 상대적 중요도를 더 명확히 이해할 수 있음


4. **가중합 계산**

   - 이 확률을 각 단어의 값 벡터와 곱하고 모두 더해 최종 출력을 만듦
   - 최종 출력은 현재 단어의 의미를 주변 단어들과의 관계를 고려하여 새롭게 표현한 것으로 단순히 단어 자체의 의미뿐만 아니라, 문맥 속에서의 의미를 포함하게 됨


5. **예시** : "은행"이라는 단어를 셀프 어텐션 메커니즘으로 처리한다고 가정
  - "은행" 주변에 "돈", "계좌" 같은 단어가 있다면, 최종 출력은 **금융기관으로서 은행**의 의미를 더 강하게 반영
  - "나무", "공원" 같은 단어가 주변에 있다면, 최종 출력은 **나무로서 은행**의 의미를 더 강하게 반영


### 셀프 어텐션의 장점
- 문장 전체 고려: 문장 내 모든 단어 쌍 사이의 관계를 직접 계산
- 병렬 처리: 모든 단어에 대해 동시에 계산이 가능
- 해석 가능성: 어텐션 가중치를 시각화하여 모델의 동작 과정을 이해할 수 있음
 


- 예시 : "나는 맛있는 사과를 먹었다"라는 문장에서 '맛있는'이라는 단어를 이해할 때
  - Transformer는 이 단어가 '사과'와 가장 관련이 깊다고 판단
  - 이렇게 각 단어마다 다른 단어들과의 관계를 계산하면, 전체 문장의 의미를 정확하게 이해할 수 있음



<br>

## 4. Transformer의 응용 분야

1. 자연어 처리 (NLP)
   - 기계 번역: 구글 번역 등에 사용되어 번역 품질을 크게 향상시킴
   - 텍스트 요약: 긴 문서를 간결하게 요약하는 데 활용됨
   - 질의응답 시스템: 주어진 질문에 대해 적절한 답변을 찾거나 생성
   - 감성 분석: 텍스트의 감정이나 의견을 분석


2. 대화 시스템
   - ChatGPT와 같은 대화형 AI의 기반 기술로 사용
   - 고객 서비스 챗봇, 가상 비서 등에 활용

3. 컴퓨터 비전
   - 이미지 캡션 생성: 이미지를 설명하는 문장을 자동으로 만들어냄
   - 객체 검출 및 세그멘테이션: 이미지 내의 객체를 식별하고 분할
   - 이미지 생성: 텍스트 설명을 바탕으로 이미지를 생성

4. 음성 처리
   - 음성 인식: 음성을 텍스트로 변환
   - 텍스트 음성 변환: 텍스트를 자연스러운 음성으로 변환

5. 추천 시스템
   - 사용자의 행동 시퀀스를 분석하여 개인화된 추천을 제공


6. 시계열 예측
   - 금융 시장 추이, 기상 환경 변화 등 복잡한 조건 예측에 활용


<br>
<br>

# 💡Today I Thought

## 오늘의 체크리스트
- [x] 알고리즘 코드카타 261-265
- [x] SQL 코드카타 88-89
- [x] TIL 작성

## 회고
&nbsp; 오늘은 코드카타랑 TIL만 작성했다. 노는게 젤 조와..🐧